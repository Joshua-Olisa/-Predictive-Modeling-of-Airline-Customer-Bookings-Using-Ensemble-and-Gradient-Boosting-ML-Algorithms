# Predictive Modelling of Customer Bookings

---

## Table of Contents

1. [About](#about)
2. [Main Aim](#main-aim)
3. [Tools & Techniques](#tools--techniques)
4. [Data Exploration & Preprocessing Summary](#data-exploration--preprocessing-summary)
5. [Modeling Approach](#modeling-approach)
6. [Results (Key Metrics)](#results-key-metrics)
7. [Takeaways](#takeaways)
8. [Recommendations](#recommendations)
9. [Future Work](#future-work)
10. [Contact / Author](#contact--author)

---

# About

This project contains a Jupyter notebook that demonstrates a full pipeline for **predictive modelling of customer bookings**. The notebook walks through exploratory data analysis (EDA), feature engineering, handling categorical variables, class imbalance techniques, model training and evaluation, and final recommendations.

### Description of the task

Given historical booking-related data, the task was to build a classifier that predicts whether a customer (or session) will result in a **booking** (the positive class). The problem is treated as a binary classification problem with an imbalanced target.

### Main aim of the project

* Produce a robust predictive model that can identify booking events (the positive class) with strong recall while balancing precision to reduce false positives.
* Demonstrate preprocessing strategies and modelling techniques suitable for imbalanced classification problems.
* Provide actionable insights and recommendations that could inform a production-ready model or a business decision-making process.

---

# Tools & Techniques

The notebook uses standard Python data science libraries and a set of specific techniques for this problem:

* **Data manipulation & EDA**: `pandas`, `numpy`
* **Exploratory steps used**: `.head()`, `.info()`, `.describe()`, distribution checks and basic visual inspections
* **Encoding**: One-hot Encoding and K-Fold Target Encoding (a cross-validated mean encoding / target encoding approach)
* **Modeling libraries**: `scikit-learn`
* **Machine Learning algorithms Used**: Random Forest, Random Forest with SMOTE and SMOTE-ENN variations (to address class imbalance), LightGBM and XGBoost
* **Model evaluation**: classification report (precision / recall / f1), confusion matrix, ROC-AUC and threshold-based adjusted accuracy

---

# Data Exploration & Preprocessing Summary

* The notebook begins with standard EDA steps to inspect column names, types and distributions using `.head()`, `.info()`, and `.describe()`.
* Missing values and datatype conversions are identified and treated.
* Categorical columns with less complex level are handled with K-Fold target encoding to avoid information leakage while capturing signal.
* Categorical columns with many levels are handled with K-Fold target encoding to avoid information leakage while capturing signal while colums with less complex levels were handle using ONe-Hot Encoding.
* Class imbalance is present (many more non-bookings than bookings), so resampling strategies (SMOTE family methods and SMOTE-ENN) are applied before modeling experiments.

---

# Modeling Approach

* Feature engineering and careful encoding (K-Fold target encoding) are applied to categorical some columns and One-Hot encoding to others prior to the model training.
* After the first results where achieved. Resampling techniques (SMOTE or SMOTE-ENN) were considered and used to rebalance the classes for training.
* XGBoost is trained with cross-validation. Models are evaluated using a range of metrics and by selecting an operating threshold when appropriate.

---

# Results (Key Metrics)

These figures and summaries are taken from the notebook results and model evaluation outputs:

* **XGBoost Results (which were the best results from all the models trained): **0.7298**
* **Accuracy (with optimal threshold)**: **0.7298**
* **AUC-ROC**: **0.7274**

**Classification report (selected rows)**

* Precison: Measures the accuracy of positive predictions (Of all the instances predicted as positive, how many were actually positive)
* Recall: Measures the model's ability to find all positive instances (Of all the actual positive instances, how many were correctly predicted as positive?)
* F1 Score: Simply the Harmonic Average of Precision and Recall
* Support: Indicates the number of actual occurrences of each class in the datase

* **Class 0 (Non-booking)**

  * Precision: **0.94**
  * Recall: **0.73**
  * F1-score: **0.82**
  * Support: **8504**

* **Class 1 (Booking)** â€” *business class of interest*

  * Precision: **0.34**
  * Recall: **0.72**
  * F1-score: **0.44**
  * Support: **1496**

> Interpretation summary: the model is effective at identifying non-bookings (high precision and decent recall for class 0). For bookings (class 1) the model has **high recall (72%) but low precision (34%)**, meaning it captures many of the true bookings but also produces many false positives. While the model addresses the class imbalance found in the dataset well with parameter tuning and threshold optimization, **precision improvements for the minority class** and further robustness require additional data, model enhancements, and continual evaluation.

---

# Takeaways

1. **Class imbalance is a primary challenge.** There are substantially more negative examples than positive ones; this pushes many standard metrics to appear favorable for the majority class.
2. **Good discrimination but low precision on the booked class.** With AUC \~0.73 and recall for bookings \~72%, the model separates classes reasonably well but produces many false positives (low precision). This implies additional precision-focused work is needed if false positives are costly.
3. **Resampling helps but doesn't fully solve precision.** SMOTE and SMOTE-ENN improved recall and class balance but precision remains low on the minority class in the experiments performed.

---

# Recommendations

*Balancing business needs (detecting bookings) against operational costs (false positives):*

1. **Tune the decision threshold with business costs in mind.** The notebook already shows threshold-based adjusted accuracy; calibrate the threshold to trade off precision vs recall according to the cost of false positives vs false negatives.
2. **Improve feature engineering.** Add behavioral/timing features, session- or user-level aggregates, time-to-event variables, or interaction features that might better distinguish true booking intent.
3. **Feature selection and regularization.** Remove noisy features or apply feature selection to reduce overfitting and improve generalization, which can help precision.
4. **Evaluate with business metrics.** Report confusion-matrix-derived business KPIs (e.g., cost per false positive, revenue per true positive) and optimize for them.

---

# Future Work

* **Expand dataset & features**: collect more labeled events or enrich data with external signals (e.g., marketing touchpoints, user history).
* **A/B testing**: evaluate model-driven interventions or threshold changes in controlled experiments to quantify business impact.
* **Automated hyperparameter search**: run Bayesian or SMBO tuning at scale to squeeze extra performance from the modeling step.

---

# Contact / Author

If you want changes to this README, additional summary slides, or a condensed one-page executive summary, please ask and I will update the document.

---

*Generated from the attached Jupyter notebook and the notebook outputs contained within.*

